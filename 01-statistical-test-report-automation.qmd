---
title: "Statistical Testing Results"
subtitle: "Company ABC"  # Add company name here
author: "Zion & Zion"
date: last-modified
date-format: "MM-DD-YYYY"
description: "A/B Testing Results Evaluation"
title-block-banner: "#FFFFFF"
callout-icon: false
callout-appearance: simple
format: 
  html:
    self_contained: true
    page-layout: full
    embed-resources: true
    smooth-scroll: true
    fontcolor: black
    toc: true
    toc-location: left
    toc-title: Summary
    toc-depth: 3
theme:
  - journal
  - styles/theme-custom.scss
css: styles/styles.css
  
params:
  
  # BAYES TEST INPUTS HERE
  sample_size_A: 8500
  success_A: 1300
  sample_size_B: 8500
  success_B: 1410
  
  # FREQUENTIST TEST INPUTS HERE
  # TODO: Revise code to use only 1 set of inputs for both methods ###
  
  # TEST SETTINGS HERE
  hypothesis: "two.sided" # one.sided or two.sided
  confidence_level: 0.95  # 0.90, 0.95, 0.99
  
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
# install.packages("BayesFactor")
library(BayesFactor)
library(tidyverse)
theme_set(theme_minimal())

# Using a uniform prior Beta(1,1)
prior <- c(1, 1)

# Calculating posterior distributions
post_A <- c(params$success_A + prior[1], params$sample_size_A - params$success_A + prior[2])
post_B <- c(params$success_B + prior[1], params$sample_size_B - params$success_B + prior[2])

# Simulating from the posterior distributions
set.seed(123)
sim_A <- rbeta(10000, post_A[1], post_A[2])
sim_B <- rbeta(10000, post_B[1], post_B[2])

# Probability that B is better than A
prob_B_better_than_A <- mean(sim_B > sim_A)

# Conversion rates
rates <- c(A = params$success_A / params$sample_size_A, B = params$success_B / params$sample_size_B)

# Relative uplift in conversion rate
relative_uplift_conversion_rate <- (rates["B"] - rates["A"]) / rates["A"]

test_type <- ifelse(params$hypothesis == "one.sided", "One-sided", "Two-sided")
```

```{r include=FALSE}
max_x_lim <- max(max(sim_A), max(sim_B))
min_x_lim <- min(min(sim_A), min(sim_B))

# Create a sequence of conversion rates
x <- seq(0, 1, length.out = 1000)

# Calculate densities
density_A <- dbeta(x, post_A[1], post_A[2])
density_B <- dbeta(x, post_B[1], post_B[2])

# Create a data frame for plotting
df <- data.frame(x, Group_A = density_A, Group_B = density_B)
```

```{r include=FALSE}
rate_A <- rates["A"]
rate_B <- rates["B"]

# Pooled conversion rate
overall_rate <- (params$success_A + params$success_B) / (params$sample_size_A + params$sample_size_B)

# Z-Score for p-value calculation
z_score <- (rate_B - rate_A) / sqrt(overall_rate * (1 - overall_rate) * (1/params$sample_size_A + 1/params$sample_size_B))

# If params$hypothesis == "one.sided" then: pnorm(z_score, lower.tail = FALSE)
# else 2 * pnorm(z_score)
# p_value <- ifelse(params$hypothesis == "one.sided", pnorm(z_score, lower.tail = FALSE), 2 * pnorm(z_score))

if (params$hypothesis == "two.sided") {
      p_value <- 2 * pnorm(-abs(z_score)) # Use abs() to handle both positive and negative z-scores
    } else if (params$hypothesis == "one.sided") {
      if (z_score < 0) {
        p_value <- pnorm(z_score, lower.tail = FALSE) # For negative z-scores, no need for lower.tail = FALSE
      } else {
        p_value <- pnorm(z_score, lower.tail = FALSE) # For positive z-scores in a one-sided test
      }
    } else {
      p_value <- NULL
    }

# alpha threshold
alpha_value <- 1 - params$confidence_level

# Test outcome 1 = reject null hypothesis, 0 = fail to reject null hypothesis
test_outcome <- ifelse(p_value < alpha_value, 1, 0)

# Z-Score for 90% Confidence Level (One-Sided)
z_score_CL <- qnorm(params$confidence_level)

# Confidence Interval for Variation A
se_A <- sqrt(rate_A * (1 - rate_A) / params$sample_size_A)
ci_lower_A <- rate_A - z_score_CL * se_A
ci_upper_A <- rate_A + z_score_CL * se_A

# Confidence Interval for Variation B
se_B <- sqrt(rate_B * (1 - rate_B) / params$sample_size_B)
ci_lower_B <- rate_B - z_score_CL * se_B
ci_upper_B <- rate_B + z_score_CL * se_B

# Dynamic callout
# callout       <- ifelse(test_outcome == 1, "tip", "important")

# Revise to take into account negative left
callout <- case_when(
  test_outcome == 1 & relative_uplift_conversion_rate > 0 ~ "tip",
  test_outcome == 1 & relative_uplift_conversion_rate < 0 ~ "important",
  test_outcome == 0                                       ~ "important",
)

call_out_text <- case_when(
  test_outcome == 1 & relative_uplift_conversion_rate > 0 ~ "The difference is large enough to declare a significant winner. There is evidence of a difference in performance between A and B.",
  test_outcome == 1 & relative_uplift_conversion_rate < 0 ~ "There is evidence of a difference in performance between A and B. However, the performance of variation B is worse than variation A.",
  test_outcome == 0                                       ~ "is not big enough to declare a significant winner. There is no real difference in performance between A and B or you need to collect more data.",
)

# call_out_text <- ifelse(test_outcome == 1, "is large enough to declare a significant winner. There is evidence of a difference in performance between A and B.", "is not big enough to declare a significant winner. There is no real difference in performance between A and B or you need to collect more data." )
```

## Test Results


::: {.callout-`r callout`} 
## `r ifelse(test_outcome == 1, "Significant Test Result!", "The test result is not significant!")`

The observed difference in conversion rates between the two variations is (`r scales::percent(relative_uplift_conversion_rate, accuracy = 0.01)`). `r call_out_text`
:::


**Test parameters:**

| Lift (%)                                                              |               Confidence Level               |   Test Type   |    P-value    |  Z-score   |
|---------------|:-------------:|:-------------:|:-------------:|:--------------|
| `r scales::percent(relative_uplift_conversion_rate, accuracy = 0.01)` | `r scales::percent(params$confidence_level)` | `r test_type` | `r round(as.double(p_value, length = 4), 4)` | `r round(as.double(z_score, length = 4), 4)` |

: {.hover}

<br>

::: panel-tabset
## Bayesian Test

**The expected distributions of variation A and B.**

```{r echo=FALSE, warning=FALSE, message=FALSE, out.width = "100%", fig.align = "center", fig.height=2.5}
df |>
  ggplot(aes(x)) +
  geom_area(aes(y = Group_A), fill = "#2E465F", alpha = 0.45) +  # Using geom_area for Group A
  geom_area(aes(y = Group_B), fill = "#D81B60", alpha = 0.45) +  # Using geom_area for 
  geom_vline(xintercept = rate_A, color = "#2E465F", linetype = "dashed", linewidth = 0.35) +
  geom_vline(xintercept = rate_B, color = "#D81B60", linetype = "dashed", linewidth = 0.35) +
  annotate("text", x = rate_A, y = 50, label = paste0("CR Control: ", round(rate_A*100, 1), "%"), color = "#2E465F", size = 2.2, vjust = 1.5, angle = 90) +
  annotate("text", x = rate_B, y = 50, label = paste0("CR Treatment: ", round(rate_B*100, 1), "%"), color = "#D81B60", size = 2.2, vjust = 1.5, angle = 90) +
  labs(
    title = NULL,
    x     = NULL, 
    y     = NULL
  ) +
  scale_x_continuous(limits = c(min_x_lim, max_x_lim), labels = scales::percent) +
  scale_y_continuous(limits = c(0, 110)) +
  theme(
    legend.position = "top",
    panel.grid  = element_blank(),
    axis.text.x = element_text(size = 7),
    axis.text.y = element_blank()
    )
```

## Frequentist Test

**Confidence Level:** `r scales::percent(params$confidence_level)`

Control confidence interval is `r round(ci_lower_A, 3)*100`% to `r round(ci_upper_A, 3)*100`%.

Treatment confidence interval is `r round(ci_lower_B, 3)*100`% to `r round(ci_upper_B, 3)*100`%.

```{r include=FALSE}
# Define labels and errors for the confidence intervals
labels <- c('Control', 'Treatment')
conversion_rates <- c(rate_A, rate_B)
ci_lower_bounds <- c(ci_lower_A, ci_lower_B)
ci_upper_bounds <- c(ci_upper_A, ci_upper_B)
errors <- c(ci_upper_A - rate_A, ci_upper_B - rate_B)

# Create a data frame for plotting
data <- data.frame(
  Variation = factor(labels, levels = rev(labels)),
  ConversionRate = conversion_rates,
  CIError = errors,
  Lower = ci_lower_bounds,
  Upper = ci_upper_bounds
)

max_y_lim <- max(data$Upper)
```


```{r echo=FALSE, warning=FALSE, message=FALSE, out.width = "100%", fig.align = "center", fig.height=2.1}
my_colors <- c("#2E465F", "#D81B60")

# Create the horizontal bar plot with confidence intervals
p <- ggplot(data, aes(x = Variation, y = ConversionRate)) +
  geom_bar(stat = "identity", aes(fill = Variation), position = position_dodge(),
           show.legend = FALSE, alpha = 0.45) +
  geom_errorbar(aes(ymin = ConversionRate - CIError, ymax = ConversionRate + CIError), width = 0.15, alpha = 0.8) +
  coord_flip() +  # Flip the coordinates to make the bars horizontal
  labs(
    title = NULL,
    x     = NULL, y = NULL
  ) + 
  # ylim(0, max_y_lim *1.15) +
  scale_y_continuous(labels = scales::percent, limits = c(0, max_y_lim *1.3)) +
  theme_minimal() +
  theme(
    legend.position = "none",
    panel.grid      = element_blank(),
    axis.text.y    = element_text(size = 7),
    axis.text.x    = element_text(size = 7),
    ) +
    scale_fill_manual(values = my_colors)

# Adding the confidence intervals as text on the bars
p <- 
  p + geom_text(
    aes(x = Variation, y = ConversionRate + CIError,
        label = paste(sprintf('%.1f%%', Lower * 100), '-', sprintf('%.1f%%', Upper * 100))),
    position = position_dodge(0.9),
    hjust = -0.2,
    alpha = 0.85,
    size = 2.5)

# Print the plot
p
```
:::

<br>

## Test Inputs & Specifications


| Variation |       Sample Size        |     Conversions      |                      Conversion Rate                       |
|------------------|:----------------:|:----------------:|:------------------:|
| Control   | `r params$sample_size_A` | `r params$success_A` | `r round(params$success_A / params$sample_size_A, 3)*100`% |
| Treatment | `r params$sample_size_B` | `r params$success_B` | `r round(params$success_B / params$sample_size_B, 3)*100`% |

: {.hover}


